# Shift-T: High-Quality PySpark Coding Standards

## Environment
- **Target Platform**: Databricks (Lakehouse Architecture)
- **Spark Version**: Databricks Runtime 13.3+ LTS
- **Language**: PySpark / Spark SQL

## Key Architectural Rules (MANDATORY)
1. **Idempotency via MERGE**:
   - All Delta Lake writes MUST use the `MERGE INTO` statement for target tables.
   - Using `.mode("overwrite")` or `.mode("append")` is forbidden unless explicitly justified for Bronze/Raw layers.
2. **Data Integrity (Unknown Handling)**:
   - All Lookups/Joins against dimension tables must handle misses.
   - Use `COALESCE(col, -1)` (or appropriate surrogate key) to avoid NULLs in Fact tables.
3. **High-Fidelity Casting (STRICT)**:
   - Every column cast must explicitly match the Destination DDL.
   - **MANDATORY**: You must iterate through the Target Schema and apply `.withColumn(col, col.cast(type))` for EVERY column before writing.
   - Use `Decimal(18,2)`, `Long`, `Boolean` precisely. Do not rely on Spark's auto-inference.
4. **Stable Surrogate Keys (IDEMPOTENT)**:
   - **Dimensions**: Use the "Lookup + New" pattern. Read target, Join, Preserve existing SKs, Generate new SKs only for new rows.
   - **Facts**: Must generate keys deterministically. DO NOT comment out key generation logic.
5. **Path & Environment Management**:
   - Paths MUST be externalized. Assume a standard `context.get_path('layer', 'table')` pattern.

## Code Structure (Medallion Standard)
```python
def execute_task(spark, context):
    """
    Principal Engineer Transpilation
    """
    # 1. PARAMETERS (from context)
    target_table = context['target']
    business_keys = context['business_keys'] # e.g. ["OrderID", "LineID"]
    
    # 2. EXTRACT (Silver/Bronze)
    # df_source = spark.table(...)
    
    # 3. TRANSFORM (Intention-based logic)
    # df_transformed = spark.sql(""" SELECT ... """)

    # 3.1 STABLE KEY GENERATION (Mandatory for Dimensions)
    # df_joined = df_transformed.join(target, "BusinessKey", "left")
    # df_final = df_joined.withColumn("SK", coalesce(target.SK, max_sk + row_number()))

    # 3.2 TYPE SAFETY LOOP (Mandatory)
    # for field in schema: df_final = df_final.withColumn(field.name, col(field.name).cast(field.type))
    
    # 4. LOAD (High-Quality Idempotent Merge)
    # Using DeltaTable.merge() or spark.sql("MERGE INTO ...")
    
    return True
```

## Optimizations
- **Z-Order/Liquid Clustering**: Include comments for post-load optimization hints.
- **Broadcast Hints**: Explicitly use `F.broadcast()` for small lookup tables.
