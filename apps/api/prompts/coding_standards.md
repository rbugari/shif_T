# Shift-T: PySpark Coding Standards & conventions

## Environment
- **Target Platform**: Databricks / Spark 
- **Spark Version**: Databricks Runtime 15.4 LTS (or 13.3 LTS as fallback)
- **Language**: PySpark (Python 3.11+)

## Architecture & Data Handling
- **Path Management**: 
  - All storage paths (Bronze, Silver, Gold) must be externalized.
  - Assume a configuration module `shiftt_config` or variable `PATHS` exists.
  - Use Mount Points or Unity Catalog volumes (e.g., `/mnt/data/silver/...`).
- **Preference: SQL Over DSL**:
  - For complex transformations, **prefer `spark.sql(""" ... """)`**. It is more readable for analysts and closer to the original SSIS SQL logic.
  - Use DataFrames for input/output and simple filtering/joining if cleaner.
- **Naming Conventions**:
  - Variable names: `camelCase` or `snake_case` (follow client preference, default to `snake_case`).
  - Output tables: Follow `catalog.schema.table_name`.

## Structure of a Generated File
Every transpiled task should follow this skeleton:

```python
# Task: [TaskName]
# Source: [Original Component Name]

def execute_task(spark, context):
    """
    Generated by Shift-T Agent C
    """
    # 1. Setup paths from context
    src_path = context.get('source_path')
    tgt_path = context.get('target_path')
    
    # 2. Extract (prefer SQL for readability)
    # spark.read... or spark.sql("SELECT * FROM ...")
    
    # 3. Transform
    # result_df = spark.sql(\"""
    #     SELECT ... FROM ...
    # \""")
    
    # 4. Load
    # result_df.write.format("delta").mode("overwrite").save(tgt_path)
    
    return True
```

## Client/Technology Overrides
- **Client A**: Prefers specific logging libraries.
- **Client B**: Uses ADLS Gen2 direct pass-through instead of mounts.
*(Agent C should check the provided 'solution_context' for these overrides)*
