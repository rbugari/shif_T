# Fase 2: Drafting & Code Generation (Gu√≠a Completa)

## üìå Introducci√≥n
La fase de **Drafting (Stage 2)** es el coraz√≥n de la modernizaci√≥n autom√°tica en Shift-T. Una vez que el triaje ha definido QU√â migrar, esta fase se encarga de C√ìMO migrarlo. Aqu√≠ transformamos la l√≥gica de negocio atrapada en paquetes legacy (SSIS .dtsx) en c√≥digo moderno, limpio y ejecutable para **Databricks (PySpark)**.

---

## üë®‚Äçüíª Para el Usuario: ¬øQu√© obtengo?

En esta etapa, ver√°s c√≥mo tu malla de procesos se ilumina nodo por nodo a medida que la IA genera el c√≥digo.

### Entregables Clave
1.  **Notebooks PySpark (.py):** C√≥digo listo para producci√≥n. No son "esqueletos"; incluyen:
    *   **Conectividad Real:** Uso de JDBC con gesti√≥n de secretos (`dbutils.secrets.get`) para fuentes SQL.
    *   **L√≥gica de Negocio:** Transformaciones, Lookups y Joins convertidos a Dataframes.
    *   **Patrones de Escritura:** Implementaci√≥n autom√°tica de **SCD Type 2** (Merge), manejo de **Surrogate Keys** e historizaci√≥n.
2.  **Reportes de Auditor√≠a (.json):** Cada script viene acompa√±ado de un "bolet√≠n de notas" generado por el Agente Cr√≠tico, que eval√∫a la calidad, seguridad y performance del c√≥digo (Score 0-100).

---

## ‚öôÔ∏è Arquitectura T√©cnica: El Pipeline de Generaci√≥n

La generaci√≥n no es una "caja negra". Es un proceso orquestado de varios pasos que garantiza precisi√≥n y especificidad.

### 1. Topology Service (El Traductor de Metadatos)
Antes de escribir una l√≠nea de c√≥digo, el sistema "desarma" el paquete SSIS original.
*   **Extracci√≥n de SQL Real:** Recupera las consultas `SELECT` exactas embebidas en los componentes de origen.
*   **Mapeo de Tipos:** Identifica las columnas de entrada y salida y sus tipos de datos originales.
*   **Detecci√≥n de Patrones:** Reconoce si el flujo es una carga incremental, un Full Load o una dimensi√≥n variante.

### 2. Migration Orchestrator (El Director)
Prepara el "Task Definition" para los agentes. Empaqueta el contexto necesario (Inputs, Outputs, Lookups, SQL Commands) en un formato estructurado que elimina la ambig√ºedad.
*   **Auto-Provisionamiento:** El orquestador ahora se encarga de crear autom√°ticamente la estructura de directorios necesaria (`Drafting/`) antes de iniciar la generaci√≥n, evitando errores de "Path not found".

### 3. Agent C: The Developer (El Constructor)
Es el experto en PySpark. Recibe la definici√≥n de la tarea y aplica las **Reglas de Oro de Shift-T**:
*   **Nunca Inventar:** Usa estrictamente los nombres de tablas y columnas proporcionados.
*   **Seguridad Primero:** Genera c√≥digo de conexi√≥n usando *Secret Scopes*, nunca credenciales hardcodeadas.
*   **Estandarizaci√≥n:** Aplica formatos de lectura (`.format("jdbc")`) y escritura (`.format("delta")`) consistentes.

### 4. Agent F: The Critic (El Auditor)
Ning√∫n c√≥digo llega al usuario sin pasar por este filtro. El Agente F act√∫a como un *Senior Data Engineer*:
*   **Valida:** ¬øEl c√≥digo implementa correctamente la l√≥gica de negocio?
*   **Audita:** ¬øCumple con las reglas de plataforma (ej: manejo de nulos, claves generadas)?
*   **Rechaza:** Si el c√≥digo es un "placeholder" o inseguro, lo bloquea y solicita regeneraci√≥n.

---

## üöÄ Caracter√≠sticas Destacadas (State 2 Functional)

### üîó Conectividad Expl√≠cita
El sistema detecta autom√°ticamente cuando una fuente es una base de datos SQL y genera el c√≥digo de conexi√≥n JDBC completo:
```python
# Ejemplo Generado
jdbc_url = dbutils.secrets.get(scope="jdbc-secrets", key="hr_db_url")
df_source = spark.read.format("jdbc").option("dbtable", f"({source_query}) as src")...
```

### ‚è≥ Slowly Changing Dimensions (SCD Type 2)
Para tablas de dimensiones (`Dim*`), el sistema genera autom√°ticamente l√≥gica `MERGE` compleja para manejar la historizaci√≥n:
*   Detecci√≥n de cambios en atributos.
*   Cierre de registros antiguos (`EndDate`, `IsCurrent = False`).
*   Inserci√≥n de nuevos registros.
*   Manejo de **Miembros Desconocidos** (-1).

### üîë Gesti√≥n de Identidad
Implementaci√≥n de l√≥gica para **Surrogate Keys** secuenciales, asegurando integridad referencial en el Lakehouse.

---

## üìä Flujo de Trabajo T√≠pico

1.  **Aprobar Triage:** Al finalizar la Fase 1, bloqueas el alcance (`Lock Scope`).
2.  **Ejecutar Pipeline:** Desde el Dashboard, inicias la migraci√≥n (`Execute Pipeline`).
3.  **Monitoreo y Persistencia:**
    *   **Live Logs:** Observas el progreso en tiempo real en la consola.
    *   **Log Persistence:** Si recargas la p√°gina, el sistema recupera autom√°ticamente el historial de ejecuci√≥n (`GET /logs`), restaurando el estado de progreso al 100% si la migraci√≥n termin√≥ exitosamente.
4.  **Aprobar:** Una vez completado, el bot√≥n "Approve & Refine" se habilita, permitiendo avanzar a la Fase 3 con un solo clic.
5.  **Descarga:** Obtienes una soluci√≥n completa organizada en carpetas.


## üîÆ Roadmap to Excellence: Qu√© falta para el "Gold Standard"

Aunque el c√≥digo actual es funcional y seguro, para alcanzar un nivel **Enterprise-Grade** absoluto, evaluamos implementar las siguientes capas en futuras iteraciones:

### 1. Test Driven Generation (TDG)
*   **Actual:** Generamos el script de transformaci√≥n.
*   **Futuro:** Generar un archivo compa√±ero `test_dim_employee.py` usando `pytest` o `chispa`.
    *   *Objetivo:* Validar l√≥gica de negocio unitaria antes del despliegue.

### 2. Data Quality & Contracts
*   **Actual:** Auditor√≠a de c√≥digo est√°tico (Agent F).
*   **Futuro:** Inyecci√≥n de checks de calidad en tiempo de ejecuci√≥n (Great Expectations / Soda).
    *   *Ejemplo:* `assert df.filter(col("pk").isNull()).count() == 0` antes de escribir.

### 3. Observabilidad Estructurada
*   **Actual:** Logs b√°sicos (`print`).
*   **Futuro:** Implementaci√≥n de un Logger estandarizado que emita m√©tricas a Azure Monitor / CloudWatch.
    *   *M√©tricas:* `rows_read`, `rows_written`, `execution_time_ms`.

### 4. Desacople de Configuraci√≥n
*   **Actual:** Variables definidas al inicio del notebook.
*   **Futuro:** Extracci√≥n de "n√∫meros m√°gicos" y umbrales a un archivo de configuraci√≥n (`config.json` o Databricks Widgets) para cambiar par√°metros sin tocar el c√≥digo.

### 5. CI/CD Pipeline as Code
*   **Actual:** Archivos `.py` en carpeta.
*   **Futuro:** Generaci√≥n autom√°tica de `azure-pipelines.yaml` o `github-workflows.yaml` para desplegar y testear estos notebooks autom√°ticamente.

---
*Shift-T Documentation Framework v1.1 - Stage 2*
