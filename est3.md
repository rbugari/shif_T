Especificación Técnica: Estadio 3 - Refinamiento y Modernización (shif_T)1. Visión GeneralEl Estadio 3 representa la fase de Ingeniería de Software de Datos. Su objetivo es tomar el código Python funcional generado en el Estadio 2 (que replica la lógica SSIS 1:1) y refactorizarlo bajo el patrón Medallion Architecture (Bronze, Silver, Gold) utilizando los estándares de Databricks Runtime 13.3+ y Unity Catalog.2. Contrato de Entrada y SalidaEntrada (Input):Conjunto completo de archivos .py (Estadio 2).Archivo platform_spec.json (Reglas de plataforma/Q&A).schema_reference.json (Contexto de DDLs e integridad).Salida (Output):Repositorio modular de Notebooks/Scripts organizados por capas.Módulo central de configuración (config.py) y utilidades (utils.py).Archivo de orquestación de Workflows.3. Definición de Agentes y Micro-servicios3.1 Agente: The Global Profiler (Analista de Activos)Este agente realiza una lectura "cross-package" para detectar redundancias.Funciones Críticas:Detección de Conexiones: Agrupa todas las cadenas JDBC detectadas en los archivos .py para generar una única entrada en el gestor de secretos.Identificación de Fuentes de Verdad: Si 5 paquetes leen de la tabla Sales.Customers, el agente marca esa tabla como un "Candidato de Ingesta Única (Bronze)".Mapeo de Dependencias: Identifica qué tablas Gold dependen de qué procesos Silver para validar el orden de ejecución.3.2 Agente: The Medallion Architect (Diseñador Estructural)Es el "Cerebro" que decide la fragmentación del código basado en el paradigma ELT (Extract-Load-Transform).Lógica de Segmentación:Layer Bronze (Ingesta): Aísla el código de lectura JDBC. Crea scripts de "paso directo" con metadatos de auditoría (_ingestion_timestamp, _source_system).Layer Silver (Calidad): Identifica el bloque de transformación de tipos (Casting), limpieza de nulos y lógica de dimensiones (SCD). Implementa el patrón de "Unknown Member" (ID -1).Layer Gold (Consumo): Identifica la unión de múltiples orígenes (Joins complejos de Fact tables) y el cálculo de KPIs finales.3.3 Agente: The Refactoring Specialist (Especialista Spark)Encargado de la escritura del código final optimizado.Reglas de Codificación Obligatorias:Sustitución de Credenciales: Elimina cualquier rastro de texto plano y utiliza dbutils.secrets.get().Uso de Widgets: Parametriza los notebooks para permitir cargas históricas o incrementales mediante dbutils.widgets.Unity Catalog: Forzado de nombres catalog.schema.table.Delta MERGE: Implementa la cláusula MERGE con whenMatchedUpdateAll() y whenNotMatchedInsertAll() para garantizar idempotencia.Vectorización: Sustituye bucles de control de Python por funciones nativas de pyspark.sql.functions.3.4 Agente: The Maintenance & Ops Auditor (Optimizador)Añade la capa de "Productividad" al código.Tareas de Auditoría:Inyección de comandos OPTIMIZE y ZORDER basados en las claves de negocio (Business Keys) identificadas por el Librarian.Configuración de VACUUM para control de costos de almacenamiento.Implementación de celdas de Logging que registren el conteo de filas en cada etapa del Hop (Bronze -> Silver -> Gold).4. Reglas de Negocio para la Refactorización (The Golden Rules)Componente SSISAcción en Estadio 3 (Refinamiento)OLE DB ConnectionMover a dbutils.secrets con un scope de seguridad definido.Lookup TransformationConvertir en un LEFT OUTER JOIN en la capa Silver/Gold.Data ConversionAplicar .cast() explícito usando el tipo de dato definido en el DDL del Librarian.Derived ColumnCrear una función withColumn o una vista temporal SQL.Slowly Changing DimensionImplementar lógica Delta MERGE con manejo de fechas de vigencia.Surrogate KeysUtilizar IDENTITY columns de Databricks o monotonically_increasing_id() según el volumen.5. Criterios de Validación (Compliance Check)El Estadio 3 solo se considera finalizado si el código pasa las siguientes pruebas:Prueba de Idempotencia: La segunda ejecución del script no debe alterar el conteo de registros ni duplicar datos.Prueba de Desacoplamiento: El notebook de la capa Gold debe poder ejecutarse sin errores siempre que la tabla Silver ya exista, sin necesidad de conectarse al origen SQL Server.Prueba de Gobernanza: Todas las tablas deben estar registradas en Unity Catalog bajo la jerarquía definida en el platform_spec.json.