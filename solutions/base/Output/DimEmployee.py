# COMMAND ----------
# Title: DimEmployee.dtsx
# Auto-Generated by Shift-T Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *

# 2. Reading Bronze / Source
# -- Setup JDBC connection for source query
jdbc_url = dbutils.secrets.get(scope="jdbc-secrets", key="hr_db_url")
jdbc_user = dbutils.secrets.get(scope="jdbc-secrets", key="hr_db_user")
jdbc_password = dbutils.secrets.get(scope="jdbc-secrets", key="hr_db_password")

source_query = '''SELECT empid, (firstname + ' ' + lastname) as fullname, title, city, country, address, phone FROM HR.Employees WHERE empid > 0'''

# Read source data (parameterized value replaced with 0 for migration)
df_source = (
    spark.read.format("jdbc")
    .option("url", jdbc_url)
    .option("user", jdbc_user)
    .option("password", jdbc_password)
    .option("dbtable", f"({source_query}) as src")
    .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver")
    .load()
)

# 3. Transformations (Apply Logic)
# -- Explicitly cast columns as per target schema (assumed typical DimEmployee schema)
# -- If schema is not provided, use best-practice types per platform rules

# Define target schema (update as per actual schema if available)
target_schema = StructType([
    StructField("EmployeeKey", IntegerType(), False),  # Surrogate Key
    StructField("EmpID", IntegerType(), True),
    StructField("FullName", StringType(), True),
    StructField("Title", StringType(), True),
    StructField("City", StringType(), True),
    StructField("Country", StringType(), True),
    StructField("Address", StringType(), True),
    StructField("Phone", StringType(), True),
    StructField("EffectiveFrom", TimestampType(), False),
    StructField("EffectiveTo", TimestampType(), False),
    StructField("IsCurrent", BooleanType(), False)
])

# Standardize column names and types
from pyspark.sql import functions as F

df_transformed = (
    df_source
    .withColumnRenamed("empid", "EmpID")
    .withColumnRenamed("fullname", "FullName")
    .withColumnRenamed("title", "Title")
    .withColumnRenamed("city", "City")
    .withColumnRenamed("country", "Country")
    .withColumnRenamed("address", "Address")
    .withColumnRenamed("phone", "Phone")
    .withColumn("EmpID", col("EmpID").cast(IntegerType()))
    .withColumn("FullName", col("FullName").cast(StringType()))
    .withColumn("Title", col("Title").cast(StringType()))
    .withColumn("City", col("City").cast(StringType()))
    .withColumn("Country", col("Country").cast(StringType()))
    .withColumn("Address", col("Address").cast(StringType()))
    .withColumn("Phone", col("Phone").cast(StringType()))
    .withColumn("EffectiveFrom", current_timestamp())
    .withColumn("EffectiveTo", lit("9999-12-31 23:59:59").cast(TimestampType()))
    .withColumn("IsCurrent", lit(True))
)

# 4. Surrogate Key Logic (Identity Handling)
# -- Get current max EmployeeKey from target
from pyspark.sql.window import Window

target_table = "DimEmployee"

def ensure_dim_table_exists():
    if not spark._jsparkSession.catalog().tableExists(target_table):
        # Create with unknown member
        unknown_df = spark.createDataFrame([
            (-1, -1, "Unknown", None, None, None, None, None, \
             "1900-01-01 00:00:00", "9999-12-31 23:59:59", True)
        ], schema=target_schema)
        unknown_df.write.format("delta").mode("overwrite").saveAsTable(target_table)

ensure_dim_table_exists()

df_target = spark.read.table(target_table)

max_id = df_target.agg({"EmployeeKey": "max"}).collect()[0][0]
if max_id is None:
    max_id = 0

# Assign surrogate keys
w = Window.orderBy(monotonically_increasing_id())
df_with_sk = df_transformed.withColumn(
    "EmployeeKey", row_number().over(w) + max_id
)

# 5. Unknown Member Enforcement
# -- Already ensured in table creation above

# 6. Writing to Silver/Gold (Apply Platform Pattern)
# -- SCD Type 2 Merge
from delta.tables import DeltaTable

delta_target = DeltaTable.forName(spark, target_table)

# Prepare merge condition (business key is EmpID)
merge_condition = "source.EmpID = target.EmpID AND target.IsCurrent = true"

# Prepare update and insert sets
update_set = {
    "EffectiveTo": "source.EffectiveFrom",
    "IsCurrent": "false"
}
insert_set = {
    "EmployeeKey": "source.EmployeeKey",
    "EmpID": "source.EmpID",
    "FullName": "source.FullName",
    "Title": "source.Title",
    "City": "source.City",
    "Country": "source.Country",
    "Address": "source.Address",
    "Phone": "source.Phone",
    "EffectiveFrom": "source.EffectiveFrom",
    "EffectiveTo": "source.EffectiveTo",
    "IsCurrent": "true"
}

# Perform SCD Type 2 Merge
(
    delta_target.alias("target")
    .merge(
        df_with_sk.alias("source"),
        merge_condition
    )
    .whenMatchedUpdate(
        condition="target.FullName <> source.FullName OR target.Title <> source.Title OR target.City <> source.City OR target.Country <> source.Country OR target.Address <> source.Address OR target.Phone <> source.Phone",
        set=update_set
    )
    .whenNotMatchedInsert(
        values=insert_set
    )
    .execute()
)

# 7. Optimization (Z-ORDER on EmpID)
spark.sql(f"OPTIMIZE {target_table} ZORDER BY (EmpID)")
