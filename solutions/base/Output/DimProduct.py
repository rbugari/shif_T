# COMMAND ----------
# Title: DimProduct.dtsx
# Auto-Generated by Shift-T Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *

# 2. Reading Bronze / Source
# -- Extract JDBC SQL from Inputs
# NOTE: You must set these values in your Databricks environment or via widgets
jdbc_url = dbutils.secrets.get(scope="my_scope", key="sqlserver_jdbc_url")
jdbc_user = dbutils.secrets.get(scope="my_scope", key="sqlserver_user")
jdbc_password = dbutils.secrets.get(scope="my_scope", key="sqlserver_password")

source_query = "SELECT * FROM Production.Products"
df_source = spark.read.format("jdbc") \
    .option("url", jdbc_url) \
    .option("user", jdbc_user) \
    .option("password", jdbc_password) \
    .option("dbtable", f"({source_query}) as src") \
    .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver") \
    .load()

# 3. Transformations (Apply Logic)
# -- No Lookups specified. No transformation logic specified in task. Pass-through.
df_transformed = df_source

# 4. Lookup Logic (if any)
# -- No lookups specified.

# 5. Writing to Silver/Gold (Apply Platform Pattern)
# -- SCD Type 2 not specified, so simple overwrite. If DimProduct has an identity column, handle surrogate key.
# -- Enforce Unknown Member (-1) if table is a dimension.

target_table = "dbo.DimProduct"

# -- Check if Unknown Member exists, if not, add it
unknown_member = {
    col: None for col in df_transformed.columns
}
if "ProductKey" in df_transformed.columns:
    unknown_member["ProductKey"] = -1
else:
    # If no ProductKey, add one
    df_transformed = df_transformed.withColumn("ProductKey", monotonically_increasing_id())
    unknown_member["ProductKey"] = -1

# Create DataFrame for Unknown Member
df_unknown = spark.createDataFrame([unknown_member], df_transformed.schema)

# Union Unknown Member
from pyspark.sql import DataFrame

def ensure_unknown_member(df: DataFrame, key_col: str = "ProductKey") -> DataFrame:
    if df.filter(col(key_col) == -1).count() == 0:
        return df.unionByName(df_unknown)
    else:
        return df

df_final = ensure_unknown_member(df_transformed)

# -- Write to Delta (idempotent overwrite)
df_final.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable(target_table)

# -- Optional: Optimize and Z-ORDER if ProductKey exists
if "ProductKey" in df_final.columns:
    spark.sql(f"OPTIMIZE {target_table} ZORDER BY (ProductKey)")
