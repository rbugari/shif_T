# COMMAND ----------
# Title: DimCategory.dtsx
# Auto-Generated by Shift-T Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *

# 2. Reading Bronze / Source
# -- Platform Rule: Use explicit JDBC connection for QUERY inputs
# -- NOTE: Replace <jdbc_url>, <username_key>, <password_key> with your environment's values

db_url = dbutils.secrets.get(scope="jdbc-secrets", key="category-db-url")
db_user = dbutils.secrets.get(scope="jdbc-secrets", key="category-db-user")
db_pass = dbutils.secrets.get(scope="jdbc-secrets", key="category-db-pass")

# The SSIS query uses a parameter (categoryid > ?). For migration, parameterize as needed.
categoryid_min = 0  # Set as required
sql_query = f"""
    SELECT categoryid, categoryname FROM Production.Categories
    WHERE categoryid > {categoryid_min}
"""

df_source = (
    spark.read.format("jdbc")
    .option("url", db_url)
    .option("user", db_user)
    .option("password", db_pass)
    .option("dbtable", f"({sql_query}) as src")
    .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver")
    .load()
)

# 3. Transformations (Apply Logic)
# -- Platform Rule: Use Target Schema for types (not provided, so infer from source)
# -- categoryid: INTEGER, categoryname: STRING

df_transformed = df_source \
    .withColumn("categoryid", col("categoryid").cast(IntegerType())) \
    .withColumn("categoryname", col("categoryname").cast(StringType()))

# 4. Lookup Logic (none for this task)

# 5. Writing to Silver/Gold (Apply Platform Pattern)
# -- Platform Rule: SCD Type 2 not required (no lookups, no SCD columns)
# -- Platform Rule: Identity handling (GENERATED ALWAYS AS IDENTITY) - ensure idempotency
# -- Platform Rule: Unknown Member (-1) enforcement for dimensions

target_table = "DimCategory"

def ensure_unknown_member(df, spark_table):
    # Check if Unknown Member exists
    unknown_id = -1
    if spark.catalog.tableExists(spark_table):
        df_target = spark.read.table(spark_table)
        if df_target.filter(col("categoryid") == unknown_id).count() == 0:
            # Insert Unknown Member
            unknown_row = [(unknown_id, "Unknown")]  # Adjust columns as per schema
            schema = StructType([
                StructField("categoryid", IntegerType(), False),
                StructField("categoryname", StringType(), True)
            ])
            df_unknown = spark.createDataFrame(unknown_row, schema)
            df_unknown.write.format("delta").mode("append").saveAsTable(spark_table)
    else:
        # Table does not exist, create with Unknown Member
        unknown_row = [(unknown_id, "Unknown")]
        schema = StructType([
            StructField("categoryid", IntegerType(), False),
            StructField("categoryname", StringType(), True)
        ])
        df_unknown = spark.createDataFrame(unknown_row, schema)
        df_unknown.write.format("delta").mode("overwrite").saveAsTable(spark_table)

# Ensure Unknown Member
ensure_unknown_member(df_transformed, target_table)

# Remove any incoming categoryid == -1 (do not overwrite Unknown Member)
df_final = df_transformed.filter(col("categoryid") != -1)

# Upsert logic (idempotent): Overwrite all except Unknown Member
if spark.catalog.tableExists(target_table):
    df_existing = spark.read.table(target_table)
    df_unknown = df_existing.filter(col("categoryid") == -1)
    (
        df_final.unionByName(df_unknown)
        .write.format("delta")
        .mode("overwrite")
        .option("overwriteSchema", "true")
        .saveAsTable(target_table)
    )
else:
    (
        df_final.write.format("delta")
        .mode("overwrite")
        .option("overwriteSchema", "true")
        .saveAsTable(target_table)
    )

# Optimize (Z-ORDER) if table is large
# spark.sql(f"OPTIMIZE {target_table} ZORDER BY (categoryid)")
