# COMMAND ----------
# Title: DimShipper.dtsx
# Auto-Generated by Shift-T Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *

# 2. Reading Bronze / Source
# -- Extract the SQL query from Inputs
# -- The '?' parameter must be replaced with a value or parameterized. For migration, assume 0 (all shippers)
# -- In production, parameterize as needed.
db_url = dbutils.secrets.get(scope="jdbc-secrets", key="sales-db-url")
db_user = dbutils.secrets.get(scope="jdbc-secrets", key="sales-db-user")
db_password = dbutils.secrets.get(scope="jdbc-secrets", key="sales-db-password")

source_query = "SELECT * FROM Sales.Shippers WHERE shipperid > 0"
df_source = (
    spark.read.format("jdbc")
    .option("url", db_url)
    .option("user", db_user)
    .option("password", db_password)
    .option("dbtable", f"({source_query}) as src")
    .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver")
    .load()
)

# 3. Transformations (Apply Logic)
# -- No lookups or transformations specified. If schema mapping is needed, inject here.
# -- If the target schema is not provided, infer columns from source.
df_dim = df_source

# 4. Unknown Member Handling (Dimension Table)
# -- Ensure a record with ID -1 exists. Assume 'shipperid' is the PK.
from pyspark.sql import Row
unknown_member = Row(shipperid=-1, companyname='Unknown', phone=None)

if 'shipperid' in df_dim.columns:
    df_dim = df_dim.unionByName(spark.createDataFrame([unknown_member], df_dim.schema), allowMissingColumns=True)

# 5. Surrogate Key Handling (if needed)
# -- If the target has an identity column, ensure sequence continuity.
# -- For Dim tables, usually shipperid is the business key, but if a surrogate key is needed, add it here.
# -- For now, assume shipperid is the PK.

# 6. Writing to Silver/Gold (Apply Platform Pattern)
# -- Use Delta Lake MERGE for idempotency
# -- If the table does not exist, create it

target_table = "DimShipper"

if not spark._jsparkSession.catalog().tableExists(target_table):
    df_dim.write.format("delta").mode("overwrite").saveAsTable(target_table)
else:
    # Merge on shipperid
    deltaTable = DeltaTable.forName(spark, target_table)
    (
        deltaTable.alias("t")
        .merge(
            df_dim.alias("s"),
            "t.shipperid = s.shipperid"
        )
        .whenMatchedUpdateAll()
        .whenNotMatchedInsertAll()
        .execute()
    )

# 7. Optimization (Optional)
# -- Z-ORDER on shipperid for query performance
spark.sql(f"OPTIMIZE {target_table} ZORDER BY (shipperid)")
