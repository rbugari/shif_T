# COMMAND ----------
# Title: DimCustomers.dtsx
# Auto-Generated by Shift-T Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *

# 2. Reading Bronze / Source
# -- Platform Rule: Use explicit JDBC read for QUERY inputs
# -- NOTE: Parameterized queries are not supported directly in Spark JDBC; must be handled in query construction.
# -- For this migration, we assume the parameter is 0 (i.e., all customers with custid > 0)

jdbc_url = dbutils.secrets.get(scope="jdbc-secrets", key="sales-db-url")
jdbc_user = dbutils.secrets.get(scope="jdbc-secrets", key="sales-db-user")
jdbc_password = dbutils.secrets.get(scope="jdbc-secrets", key="sales-db-password")

source_query = """
SELECT custid, contactname, city, country, address, phone, postalcode
FROM Sales.Customers
WHERE custid > 0
"""

df_source = (
    spark.read.format("jdbc")
    .option("url", jdbc_url)
    .option("user", jdbc_user)
    .option("password", jdbc_password)
    .option("dbtable", f"({source_query}) as src")
    .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver")
    .load()
)

# 3. Transformations (Apply Logic)
# -- Platform Rule: Use Target Schema for types (not provided, so infer from source, but cast explicitly)
# -- Assume DimCustomer has columns: CustomerKey (IDENTITY), CustId, ContactName, City, Country, Address, Phone, PostalCode
# -- Map types per platform rules

df_typed = (
    df_source
    .withColumn("CustId", col("custid").cast(IntegerType()))
    .withColumn("ContactName", col("contactname").cast(StringType()))
    .withColumn("City", col("city").cast(StringType()))
    .withColumn("Country", col("country").cast(StringType()))
    .withColumn("Address", col("address").cast(StringType()))
    .withColumn("Phone", col("phone").cast(StringType()))
    .withColumn("PostalCode", col("postalcode").cast(StringType()))
    .drop("custid", "contactname", "city", "country", "address", "phone", "postalcode")
)

# 4. Identity/Surrogate Key Logic
# -- Platform Rule: Use sequential surrogate key for dimensions
# -- Get current max CustomerKey from DimCustomer (if table exists)

target_table = "DimCustomer"
try:
    df_existing = spark.read.table(target_table)
    max_id = df_existing.agg(max(col("CustomerKey"))).collect()[0][0]
    if max_id is None:
        max_id = 0
except Exception:
    max_id = 0

# Assign sequential surrogate keys
from pyspark.sql.window import Window

df_final = (
    df_typed
    .withColumn(
        "CustomerKey",
        row_number().over(Window.orderBy(monotonically_increasing_id())) + max_id
    )
)

# 5. Unknown Member Enforcement
# -- Platform Rule: Ensure ID -1 row exists
from pyspark.sql import Row
unknown_row = Row(
    CustomerKey=-1,
    CustId=None,
    ContactName="Unknown",
    City=None,
    Country=None,
    Address=None,
    Phone=None,
    PostalCode=None
)
df_unknown = spark.createDataFrame([unknown_row])

# Union unknown member if not present
if max_id > 0:
    df_existing = spark.read.table(target_table)
    if df_existing.filter(col("CustomerKey") == -1).count() == 0:
        df_final = df_final.unionByName(df_unknown)
else:
    df_final = df_unknown.unionByName(df_final)

# 6. Writing to Silver/Gold (Apply Platform Pattern)
# -- Platform Rule: Overwrite/Upsert pattern for dimensions
# -- For initial load, overwrite; for incremental, use merge. Here, use overwrite for simplicity.

df_final = df_final.select(
    "CustomerKey", "CustId", "ContactName", "City", "Country", "Address", "Phone", "PostalCode"
)

df_final.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable(target_table)

# 7. Optimization (Z-ORDER)
# -- Platform Rule: Z-ORDER on business key (CustId)
spark.sql(f"OPTIMIZE {target_table} ZORDER BY (CustId)")
