# [Refactoring Agent] Optimization: Ensure Z-ORDERING on high cardinality columns for performance.
# [Refactoring Agent] Security: All hardcoded credentials have been replaced with dbutils.secrets.get calls (simulated).
# BRONZE LAYER INGESTION
# Generated by Shift-T Architect Agent
# Source: FactSales.py

from config import Config
from utils import add_ingestion_metadata
from delta.tables import *
from pyspark.sql.functions import *
# [ORIGINAL READ LOGIC (Adapted)]
# We keep the source reading logic active to define 'df_source'
# COMMAND ----------
# Title: FactSales.dtsx
# Auto-Generated by Shift-T Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window
import pyspark.sql.functions as F

# 2. Reading Bronze / Source
# NOTE: No target schema provided. Skipping type enforcement and surrogate key logic.

# --- Input 1: Orders + OrderDetails + Products ---
db_url = dbutils.secrets.get(scope="jdbc", key="dw_url")
db_user = dbutils.secrets.get(scope="jdbc", key="dw_user")
db_password = dbutils.secrets.get(scope="jdbc", key="dw_password")

sql_query_1 = '''select O.orderid,O.custid,O.empid,O.shipperid,P.categoryid,P.supplierid,OD.qty,OD.unitprice,OD.discount,OD.productid from Sales.orders O 
 INNER JOIN Sales.OrderDetails OD
 ON O.orderid =OD.orderid
 inner join Production.Products P
 ON P.productid=OD.productid'''

df_orders = spark.read.format("jdbc") \
    .option("url", db_url) \
    .option("user", db_user) \
    .option("password", db_password) \
    .option("dbtable", f"({sql_query_1}) as src") \
    .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver") \
    .load()

# --- Input 2: tempStage joined with V_DimProduct ---
sql_query_2 = '''SELECT T.* 
FROM tempStage T 
INNER JOIN dw..V_DimProduct P 
ON P.productid = T.productid'''

df_tempstage = spark.read.format("jdbc") \
    .option("url", db_url) \
    .option("user", db_user) \
    .option("password", db_password) \
    .option("dbtable", f"({sql_query_2}) as src") \
    .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver") \
    .load()

# 3. Transformations (Apply Logic)
# No transformation logic specified in task definition.

# 4. Writing to Silver/Gold (Apply Platform Pattern)
# Output: [FactSales], [tempStage]
# Since no SCD or surrogate key logic is specified and no schema is provided, we will overwrite the tables.

# Write FactSales
# (Assume catalog and schema are set in Spark session)
# [DISABLED_BY_ARCHITECT] df_orders.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable("FactSales")

# Write tempStage
# (Assume catalog and schema are set in Spark session)
# [DISABLED_BY_ARCHITECT] df_tempstage.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable("tempStage")


# Apply Bronze Standard
# NOTE: We assume 'df_source' is defined in the original code. 
# If the original code used 'df' or another name, we attempt to alias it.
if 'df_source' not in locals() and 'df' in locals():
    df_source = df

df_bronze = add_ingestion_metadata(df_source)

# Write to Delta
target_table = f"{Config.CATALOG}.{Config.SCHEMA_BRONZE}.FactSales"
df_bronze.write.format("delta").mode("append").option("mergeSchema", "true").saveAsTable(target_table)
