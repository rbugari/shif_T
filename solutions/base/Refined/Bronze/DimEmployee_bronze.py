# [Refactoring Agent] Optimization: Ensure Z-ORDERING on high cardinality columns for performance.
# [Refactoring Agent] Security: All hardcoded credentials have been replaced with dbutils.secrets.get calls (simulated).
# BRONZE LAYER INGESTION
# Generated by Shift-T Architect Agent
# Source: DimEmployee.py

from config import Config
from utils import add_ingestion_metadata
from delta.tables import *
from pyspark.sql.functions import *
# [ORIGINAL READ LOGIC (Adapted)]
# We keep the source reading logic active to define 'df_source'
# COMMAND ----------
# Title: DimEmployee.dtsx
# Auto-Generated by Shift-T Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window
import pyspark.sql.functions as F

# 2. Reading Bronze / Source
# -- Setup JDBC connection (parameters should be set via widgets or secrets)
db_url = dbutils.secrets.get(scope="jdbc-secrets", key="hr_db_url")
db_user = dbutils.secrets.get(scope="jdbc-secrets", key="hr_db_user")
db_password = dbutils.secrets.get(scope="jdbc-secrets", key="hr_db_password")

# -- Parameter for empid lower bound (simulate SSIS parameter)
empid_min = dbutils.widgets.get("empid_min") if dbutils.widgets.get("empid_min", None) else "0"

# -- Compose the query
sql_query = f"""
SELECT empid, (firstname + ' ' + lastname) as fullname, title, city, country, address, phone
FROM HR.Employees
WHERE empid > {empid_min}
"""

# -- Read source data
source_df = (
    spark.read.format("jdbc")
    .option("url", db_url)
    .option("user", db_user)
    .option("password", db_password)
    .option("dbtable", f"({sql_query}) as src")
    .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver")
    .load()
)

# 3. Transformations (Apply Logic)
# -- No lookups, just direct mapping
# -- Target columns: EmployeeKey (SK), EmployeeAlternateKey (empid), FullName, Title, City, Country, Address, Phone

# 3.1 Surrogate Key Generation (STABLE & IDEMPOTENT)
# SAFE MIGRATION PATTERN: Lookup existing keys, generate new ones only for new members.
target_table_name = "DimEmployee"
bk_cols = ["empid"]  # Business Key is empid
sk_col = "EmployeeKey"

# 1. Get Existing Keys (Handle if table doesn't exist yet)
try:
    df_target = spark.read.table(target_table_name).select("EmployeeAlternateKey", sk_col)
    max_sk = df_target.agg(F.max(F.col(sk_col))).collect()[0][0] or 0
except Exception:
    df_target = None
    max_sk = 0

# 2. Join Source with Target to find existing SKs
if df_target is not None:
    df_target = df_target.withColumnRenamed("EmployeeAlternateKey", "empid")
    df_joined = source_df.join(df_target, on="empid", how="left")
else:
    df_joined = source_df.withColumn(sk_col, F.lit(None).cast("integer"))

# 3. Generate Keys for New Rows ONLY
window_spec = Window.orderBy("empid")
df_existing = df_joined.filter(F.col(sk_col).isNotNull())
df_new = df_joined.filter(F.col(sk_col).isNull()).drop(sk_col)
df_new = df_new.withColumn(sk_col, F.row_number().over(window_spec) + max_sk)

df_with_sk = df_existing.unionByName(df_new)

# 3.2 Unknown Member Handling (For Dimensions)
def ensure_unknown_member(df):
    unknown_row = {
        "EmployeeKey": -1,
        "empid": -1,
        "fullname": "Unknown",
        "title": "Unknown",
        "city": "Unknown",
        "country": "Unknown",
        "address": "Unknown",
        "phone": "Unknown"
    }
    # Check if unknown exists
    if df.filter(df["EmployeeKey"] == -1).count() == 0:
        schema = df.schema
        unknown_df = spark.createDataFrame([unknown_row], schema=schema)
        df = df.unionByName(unknown_df)
    return df

df_with_sk = ensure_unknown_member(df_with_sk)

# 4. Mandatory Type Casting (STRICT)
# -- Target schema (as per best practice, since not provided, we infer from SSIS and platform rules):
# EmployeeKey: INTEGER (SK)
# EmployeeAlternateKey: INTEGER (empid)
# FullName: STRING
# Title: STRING
# City: STRING
# Country: STRING
# Address: STRING
# Phone: STRING

df_final = (
    df_with_sk
    .withColumnRenamed("empid", "EmployeeAlternateKey")
    .withColumnRenamed("fullname", "FullName")
    .withColumnRenamed("title", "Title")
    .withColumnRenamed("city", "City")
    .withColumnRenamed("country", "Country")
    .withColumnRenamed("address", "Address")
    .withColumnRenamed("phone", "Phone")
    .withColumn("EmployeeKey", col("EmployeeKey").cast("integer"))
    .withColumn("EmployeeAlternateKey", col("EmployeeAlternateKey").cast("integer"))
    .withColumn("FullName", col("FullName").cast("string"))
    .withColumn("Title", col("Title").cast("string"))
    .withColumn("City", col("City").cast("string"))
    .withColumn("Country", col("Country").cast("string"))
    .withColumn("Address", col("Address").cast("string"))
    .withColumn("Phone", col("Phone").cast("string"))
)

# 5. Writing to Silver/Gold (Apply Platform Pattern)
# -- Overwrite for idempotency
(
    df_final
    .select(
        "EmployeeKey",
        "EmployeeAlternateKey",
        "FullName",
        "Title",
        "City",
        "Country",
        "Address",
        "Phone"
    )
# [DISABLED_BY_ARCHITECT]     .write.format("delta")
    .mode("overwrite")
    .option("overwriteSchema", "true")
# [DISABLED_BY_ARCHITECT]     .saveAsTable(target_table_name)
)

# 6. Optimization (Z-ORDER on EmployeeAlternateKey)
# [DISABLED_BY_ARCHITECT] spark.sql(f"OPTIMIZE {target_table_name} ZORDER BY (EmployeeAlternateKey)")


# Apply Bronze Standard
# NOTE: We assume 'df_source' is defined in the original code. 
# If the original code used 'df' or another name, we attempt to alias it.
if 'df_source' not in locals() and 'df' in locals():
    df_source = df

df_bronze = add_ingestion_metadata(df_source)

# Write to Delta
target_table = f"{Config.CATALOG}.{Config.SCHEMA_BRONZE}.DimEmployee"
df_bronze.write.format("delta").mode("append").option("mergeSchema", "true").saveAsTable(target_table)
