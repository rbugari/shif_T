# COMMAND ----------
# Title: DimEmployee.dtsx
# Auto-Generated by Shift-T Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window

# 2. Reading Bronze / Source
# Securely fetch JDBC credentials
jdbc_hostname = dbutils.secrets.get('scope_hr', 'hr_db_hostname')
jdbc_port = dbutils.secrets.get('scope_hr', 'hr_db_port')
jdbc_database = dbutils.secrets.get('scope_hr', 'hr_db_name')
jdbc_username = dbutils.secrets.get('scope_hr', 'hr_db_user')
jdbc_password = dbutils.secrets.get('scope_hr', 'hr_db_password')
jdbc_url = f"jdbc:sqlserver://{jdbc_hostname}:{jdbc_port};databaseName={jdbc_database}"

# Parameter for empid threshold (replace with widget or parameter as needed)
empid_threshold = dbutils.widgets.get('empid_threshold') if 'empid_threshold' in dbutils.widgets.get('') else '0'

source_query = f"""
SELECT empid, (firstname + ' ' + lastname) AS fullname, title, city, country, address, phone
FROM HR.Employees
WHERE empid > {empid_threshold}
"""

# Read source data
source_df = (
    spark.read.format("jdbc")
    .option("url", jdbc_url)
    .option("dbtable", f"({source_query}) as src")
    .option("user", jdbc_username)
    .option("password", jdbc_password)
    .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver")
    .load()
)

# 3. Transformations (Apply Logic)
# No lookups required for this task

# 3.1 Surrogate Key Generation (STABLE & IDEMPOTENT)
# SAFE MIGRATION PATTERN: Lookup existing keys, generate new ones only for new members.
target_table_name = "DimEmployee"
bk_cols = ["empid"]
sk_col = "EmployeeKey"

# 1. Get Existing Keys (Handle if table doesn't exist yet)
try:
    df_target = spark.read.table(target_table_name).select(*bk_cols, sk_col)
    max_sk = df_target.agg(max(col(sk_col))).collect()[0][0] or 0
except Exception:
    df_target = None
    max_sk = 0

# 2. Join Source with Target to find existing SKs
if df_target is not None:
    df_joined = source_df.join(df_target, on=bk_cols, how="left")
else:
    df_joined = source_df.withColumn(sk_col, lit(None).cast("integer"))

# 3. Generate Keys for New Rows ONLY
window_spec = Window.orderBy(*bk_cols)
df_existing = df_joined.filter(col(sk_col).isNotNull())
df_new = df_joined.filter(col(sk_col).isNull()).drop(sk_col)
df_new = df_new.withColumn(sk_col, row_number().over(window_spec) + max_sk)

# 4. Union
employee_df = df_existing.unionByName(df_new)

# 3.2 Unknown Member Handling (For Dimensions)
def ensure_unknown_member(df):
    unknown_row = {
        "EmployeeKey": -1,
        "empid": -1,
        "fullname": "Unknown",
        "title": "Unknown",
        "city": "Unknown",
        "country": "Unknown",
        "address": "Unknown",
        "phone": "Unknown"
    }
    from pyspark.sql import Row
    unknown_df = spark.createDataFrame([Row(**unknown_row)])
    if df.filter(col("EmployeeKey") == -1).count() == 0:
        return df.unionByName(unknown_df)
    else:
        return df

employee_df = ensure_unknown_member(employee_df)

# 4. Mandatory Type Casting (STRICT)
# Define target schema types explicitly
employee_df = (
    employee_df
    .withColumn("EmployeeKey", col("EmployeeKey").cast("integer"))
    .withColumn("empid", col("empid").cast("integer"))
    .withColumn("fullname", col("fullname").cast("string"))
    .withColumn("title", col("title").cast("string"))
    .withColumn("city", col("city").cast("string"))
    .withColumn("country", col("country").cast("string"))
    .withColumn("address", col("address").cast("string"))
    .withColumn("phone", col("phone").cast("string"))
)

# 5. Writing to Silver/Gold (Apply Platform Pattern)
# Overwrite the DimEmployee table (idempotent)
(
    employee_df
    .write
    .format("delta")
    .mode("overwrite")
    .option("overwriteSchema", "true")
    .saveAsTable(target_table_name)
)

# 6. Optimization (Z-ORDER)
# Z-ORDER on empid for query performance
spark.sql(f"OPTIMIZE {target_table_name} ZORDER BY (empid)")
