# COMMAND ----------
# Title: My First SSIS Data Load.dtsx
# Auto-Generated by Shift-T Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *

# 2. Reading Bronze / Source
# NOTE: Replace 'source_db.source_table' with your actual source table name
# For demonstration, using 'bronze.Product_source' as a placeholder

df_source = spark.read.table("bronze.Product_source")

# 3. Transformations (Apply Logic)
# Explicitly cast all columns to match the target schema

df_transformed = df_source.select(
    col("ProductID").cast(IntegerType()).alias("ProductID"),
    col("Name").cast(IntegerType()).alias("Name"),
    col("ProductNumber").cast(StringType()).alias("ProductNumber"),
    col("Color").cast(StringType()).alias("Color"),
    col("SafetyStockLevel").cast(StringType()).alias("SafetyStockLevel"),
    col("ListPrice").cast(DecimalType(19, 4)).alias("ListPrice"),
    col("Size").cast(StringType()).alias("Size")
)

# 4. Lookup Logic (if any)
# No lookups defined in this task.

# 5. Writing to Silver/Gold (Apply Platform Pattern)
# Standard overwrite pattern for initial load (no SCD, no merge required)
# Replace 'silver.Product' with your actual target table name

df_transformed.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable("silver.Product")

# Optional: Z-ORDER optimization on high-cardinality columns (e.g., ProductID)
spark.sql("OPTIMIZE silver.Product ZORDER BY (ProductID)")
