# COMMAND ----------
# Title: Package1.dtsx - Product Table Load
# Auto-Generated by Shift-T Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *

# 2. Reading Bronze / Source
# NOTE: Replace 'source_db.Product' with actual source table/view
# Ensure all columns are selected and cast to target types as per schema

df_source = (
    spark.read.table("source_db.Product")
    .select(
        col("ProductID").cast(IntegerType()).alias("ProductID"),
        col("Name").cast(IntegerType()).alias("Name"),
        col("ProductNumber").cast(StringType()).alias("ProductNumber"),
        col("Color").cast(StringType()).alias("Color"),
        col("SafetyStockLevel").cast(StringType()).alias("SafetyStockLevel"),
        col("ListPrice").cast(DecimalType(19,4)).alias("ListPrice"),
        col("Size").cast(StringType()).alias("Size")
    )
)

# 3. Transformations (Apply Logic)
# No additional transformations specified in task.

# 4. Lookup Logic (if any)
# No lookups specified.

# 5. Writing to Silver/Gold (Apply Platform Pattern)
# Idempotent overwrite (no SCD, no PK, no incremental logic required)
# Replace 'target_db.Product' with actual target table name

df_source.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable("target_db.Product")

# 6. Optimization (Z-ORDER if applicable)
# No high-cardinality business key specified, skipping Z-ORDER.

# 7. QA: Enforce Unknown Member if required (not applicable here, as no FKs)
