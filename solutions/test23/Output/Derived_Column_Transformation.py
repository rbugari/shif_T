# COMMAND ----------
# Title: Derived_Column_Transformation.dtsx
# Auto-Generated by Shift-T Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *

# 2. Reading Bronze / Source
# NOTE: Replace 'source_db.source_product' with the actual source table/view
# Ensure all columns are read and explicitly cast to match the target schema

df_source = (
    spark.read.table("source_db.source_product")
    .select(
        col("ProductID").cast(IntegerType()).alias("ProductID"),
        col("Name").cast(IntegerType()).alias("Name"),
        col("ProductNumber").cast(StringType()).alias("ProductNumber"),
        col("Color").cast(StringType()).alias("Color"),
        col("SafetyStockLevel").cast(StringType()).alias("SafetyStockLevel"),
        col("ListPrice").cast(DecimalType(19, 4)).alias("ListPrice"),
        col("Size").cast(StringType()).alias("Size")
    )
)

# 3. Transformations (Apply Logic)
# No additional transformations specified in task logic.
df_transformed = df_source

# 4. Lookup Logic (if any)
# No lookups specified.

# 5. Writing to Silver/Gold (Apply Platform Pattern)
# Idempotent overwrite (no SCD, no PK, standard table)
# Replace 'target_db.Product' with the actual target table name

df_transformed.write.format("delta") \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .saveAsTable("target_db.Product")

# Optional: Z-ORDER optimization if ProductNumber is high-cardinality
# spark.sql("OPTIMIZE target_db.Product ZORDER BY (ProductNumber)")
