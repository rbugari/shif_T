# COMMAND ----------
# Title: UnionAll_Transformation.dtsx
# Auto-Generated by Shift-T Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *

# 2. Reading Bronze / Source
# NOTE: No explicit inputs defined. Please replace with actual source tables as needed.
# Example placeholder DataFrames (replace with real sources):
df_source1 = spark.read.table("bronze.product_source1")
df_source2 = spark.read.table("bronze.product_source2")

# 3. Transformations (Apply Logic)
# Union All Transformation: Ensure schema alignment and explicit type casting per Target Schema

def cast_to_target_schema(df):
    return (
        df
        .withColumn("ProductID", col("ProductID").cast(IntegerType()))
        .withColumn("Name", col("Name").cast(IntegerType()))
        .withColumn("ProductNumber", col("ProductNumber").cast(StringType()))
        .withColumn("Color", col("Color").cast(StringType()))
        .withColumn("SafetyStockLevel", col("SafetyStockLevel").cast(StringType()))
        .withColumn("ListPrice", col("ListPrice").cast(DecimalType(19,4)))
        .withColumn("Size", col("Size").cast(StringType()))
    )

# Apply schema casting to all sources
df1 = cast_to_target_schema(df_source1)
df2 = cast_to_target_schema(df_source2)

# Union all sources
df_union = df1.unionByName(df2)

# 4. Lookup Logic (if any)
# No lookups defined in this task.

# 5. Writing to Silver/Gold (Apply Platform Pattern)
# Idempotency: Overwrite the target table (no SCD or merge needed for Union All)

target_table = "silver.Product"
df_union.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable(target_table)

# Optional: Z-ORDER optimization on high-cardinality columns (e.g., ProductID)
spark.sql(f"OPTIMIZE {target_table} ZORDER BY (ProductID)")
